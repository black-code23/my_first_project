{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMUxQRhK76vDH7/9I7ScNV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/black-code23/my_first_project/blob/main/diseasepredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UQIc44KW6y8",
        "outputId": "d6d8ec5e-65d4-4164-faa2-1b1b068304cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification  # Or use a model type of your choice\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "DqLi9KhcXb03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'model_name' with the Hugging Face model you want to use\n",
        "model_name = \"Zabihin/Symptom_to_Diagnosis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,from_tf = True,attn_implementation=\"eager\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn7PC_KCXgn-",
        "outputId": "070346a8-40f3-4843-ac99-e473b36e8630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All TF 2.0 model weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the TF 2.0 model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text input\n",
        "text = input(\"Enter symptoms (separated by commas): \")\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")  # 'pt' specifies PyTorch tensors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "XPawGQ7Qej-8",
        "outputId": "515514c2-bdf5-4932-e901-e6deed4b4990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8d147faf65b5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example text input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter symptoms (separated by commas): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Tokenize the input text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 'pt' specifies PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "probabilities = torch.softmax(logits, dim=1)\n",
        "predicted_class_idx = probabilities.argmax().item()\n",
        "\n",
        "# Class label mapping (replace with actual labels)\n",
        "id2label = {\n",
        "    \"0\": \"allergy\",\n",
        "    \"1\": \"arthritis\",\n",
        "    \"2\": \"bronchial asthma\",\n",
        "    \"3\": \"cervical spondylosis\",\n",
        "    \"4\": \"chicken pox\",\n",
        "    \"5\": \"common cold\",\n",
        "    \"6\": \"dengue\",\n",
        "    \"7\": \"diabetes\",\n",
        "    \"8\": \"drug reaction\",\n",
        "    \"9\": \"fungal infection\",\n",
        "    \"10\": \"gastroesophageal reflux disease\",\n",
        "    \"11\": \"hypertension\",\n",
        "    \"12\": \"impetigo\",\n",
        "    \"13\": \"jaundice\",\n",
        "    \"14\": \"malaria\",\n",
        "    \"15\": \"migraine\",\n",
        "    \"16\": \"peptic ulcer disease\",\n",
        "    \"17\": \"pneumonia\",\n",
        "    \"18\": \"psoriasis\",\n",
        "    \"19\": \"typhoid\",\n",
        "    \"20\": \"urinary tract infection\",\n",
        "    \"21\": \"varicose veins\"\n",
        "}\n",
        "\n",
        "# Get predicted class label\n",
        "predicted_class_label = id2label[str(predicted_class_idx)]\n",
        "\n",
        "print(f\"Predicted Class Label: {predicted_class_label}\")\n",
        "print(f\"Confidence Score: {probabilities[0][predicted_class_idx].item():.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FYkasIwPhQyO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "397f5177-c1e9-4f8c-b01f-7912404807fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'LogisticRegression' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c442020a0d10>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredicted_class_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'LogisticRegression' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Replace 'model_name' with the Hugging Face model you want to use\n",
        "model_name = \"Zabihin/Symptom_to_Diagnosis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, from_tf=True, attn_implementation=\"eager\")\n",
        "\n",
        "# Class label mapping (replace with actual labels)\n",
        "id2label = {\n",
        "    \"0\": \"allergy\",\n",
        "    \"1\": \"arthritis\",\n",
        "    \"2\": \"bronchial asthma\",\n",
        "    \"3\": \"cervical spondylosis\",\n",
        "    \"4\": \"chicken pox\",\n",
        "    \"5\": \"common cold\",\n",
        "    \"6\": \"dengue\",\n",
        "    \"7\": \"diabetes\",\n",
        "    \"8\": \"drug reaction\",\n",
        "    \"9\": \"fungal infection\",\n",
        "    \"10\": \"gastroesophageal reflux disease\",\n",
        "    \"11\": \"hypertension\",\n",
        "    \"12\": \"impetigo\",\n",
        "    \"13\": \"jaundice\",\n",
        "    \"14\": \"malaria\",\n",
        "    \"15\": \"migraine\",\n",
        "    \"16\": \"peptic ulcer disease\",\n",
        "    \"17\": \"pneumonia\",\n",
        "    \"18\": \"psoriasis\",\n",
        "    \"19\": \"typhoid\",\n",
        "    \"20\": \"urinary tract infection\",\n",
        "    \"21\": \"varicose veins\"\n",
        "}\n",
        "\n",
        "# Get user input\n",
        "text = input(\"Enter symptoms (separated by commas): \")\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")  # 'pt' specifies PyTorch tensors\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "probabilities = torch.softmax(logits, dim=1)\n",
        "predicted_class_idx = probabilities.argmax().item()\n",
        "\n",
        "# Get predicted class label\n",
        "predicted_class_label = id2label[str(predicted_class_idx)]\n",
        "\n",
        "print(f\"Predicted Class Label: {predicted_class_label}\")\n",
        "print(f\"Confidence Score: {probabilities[0][predicted_class_idx].item():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAEy6C7wp7y6",
        "outputId": "f1e8ee1e-1348-4120-bc80-fbac44405b7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All TF 2.0 model weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the TF 2.0 model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter symptoms (separated by commas): vomiting,fever,coldness\n",
            "Predicted Class Label: dengue\n",
            "Confidence Score: 0.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Replace 'model_name' with the Hugging Face model you want to use\n",
        "model_name = \"Zabihin/Symptom_to_Diagnosis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, from_tf=True, attn_implementation=\"eager\")\n",
        "\n",
        "# Class label mapping (replace with actual labels)\n",
        "id2label = {\n",
        "    \"0\": \"allergy\",\n",
        "    \"1\": \"arthritis\",\n",
        "    \"2\": \"bronchial asthma\",\n",
        "    \"3\": \"cervical spondylosis\",\n",
        "    \"4\": \"chicken pox\",\n",
        "    \"5\": \"common cold\",\n",
        "    \"6\": \"dengue\",\n",
        "    \"7\": \"diabetes\",\n",
        "    \"8\": \"drug reaction\",\n",
        "    \"9\": \"fungal infection\",\n",
        "    \"10\": \"gastroesophageal reflux disease\",\n",
        "    \"11\": \"hypertension\",\n",
        "    \"12\": \"impetigo\",\n",
        "    \"13\": \"jaundice\",\n",
        "    \"14\": \"malaria\",\n",
        "    \"15\": \"migraine\",\n",
        "    \"16\": \"peptic ulcer disease\",\n",
        "    \"17\": \"pneumonia\",\n",
        "    \"18\": \"psoriasis\",\n",
        "    \"19\": \"typhoid\",\n",
        "    \"20\": \"urinary tract infection\",\n",
        "    \"21\": \"varicose veins\"\n",
        "}\n",
        "\n",
        "# Save the model and tokenizer\n",
        "with open('symptom_diagnosis_model.pkl', 'wb') as file:\n",
        "    pickle.dump((model, tokenizer, id2label), file)\n",
        "\n",
        "print(\"Model, tokenizer, and id2label saved to symptom_diagnosis_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MA6zGnf3C01",
        "outputId": "924815d6-0d80-40d1-bac1-d1f6fae1255a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All TF 2.0 model weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the TF 2.0 model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model, tokenizer, and id2label saved to symptom_diagnosis_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the saved model and components\n",
        "with open('symptom_diagnosis_model.pkl', 'rb') as file:\n",
        "    model, tokenizer, id2label = pickle.load(file)\n",
        "\n",
        "# ... (Rest of your prediction code using the loaded model, tokenizer, and id2label)"
      ],
      "metadata": {
        "id": "SMEqSa_Q3k-B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}